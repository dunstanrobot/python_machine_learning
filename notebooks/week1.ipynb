{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fredhutch.io -- Intermediate Python: Machine Learning\n",
    "### Fred Hutchinson Cancer Research Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 -- Basic Data Understanding and Preparation: Data Import, Cleaning, Visualization, and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have access to a set of data describing a variety of features about people's commutes. We're hoping to use these data to predict how long a new person's commute will be given some information about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can machine learning help us achieve our goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need to do with the data first in order to decide where to go and how to use machine learning thoughtfully?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA, Data import, cleaning, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Data exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The % signifies a ipynb \"magic function\". This line allows the figure to be rendered in the notebook next to the\n",
    "# code. \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What versions are you running?\n",
    "print(\"Pandas version:\",pd.__version__)\n",
    "print(\"Numpy version:\",np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data. Remember to pass in the full file path!\n",
    "training_dataset_path = '../data/commute-times-train.csv'\n",
    "testing_dataset_path = '../data/commute-times-test.csv'\n",
    "train_data_raw = pd.read_csv(training_dataset_path, index_col=[0], parse_dates=['time_of_day_ts'])\n",
    "test_data_raw = pd.read_csv(testing_dataset_path, index_col=[0], parse_dates=['time_of_day_ts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick primer to viewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to quickly access docstring on syntax\n",
    "?pd.DataFrame.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the first five rows of data.\n",
    "train_data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other options include using .tail() and .sample(). You can specify how many rows of data to see. The default is 5. \n",
    "train_data_raw.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other methods to use to get some information about your dataset. By default, using .describe() provides some summary\n",
    "# statistics of what kind of data?\n",
    "train_data_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case it is of interest, there are kwargs that will provide additional information.\n",
    "train_data_raw.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a quick way to learn what size dataset you are working with. .shape will return (rows, columns).\n",
    "train_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to know what data types are in your dataset! It's not usually a good idea to make assumptions about \n",
    "# the data you are working with. \n",
    "train_data_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.NaN != [None, 0] \n",
    "### NaN is very useful because you can leverage vectorized operations in numpy. The data type for each of those values (NaN, None, 0) is different. Think about this when you consider how to impute or otherwise handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missing data in this dataset?\n",
    "train_data_raw.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are NaNs, how many are present in each column?\n",
    "train_data_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's continue learning about the data through visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we discern any information from lat long?\n",
    "train_data_raw.plot.scatter(x='source_latitude', y='source_longitude', marker='.')\n",
    "plt.title('Source of Commute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw.plot.scatter(x='destination_latitude', y='destination_longitude', marker='.')\n",
    "plt.title('Destination of Commute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show the distribution of commute times.\n",
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['commute_time'].hist(bins=50)\n",
    "plt.title('Histogram of Commute Times')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a nice summary table, but let's graph this out somehow.\n",
    "train_data_raw.groupby('commute_type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw.groupby('commute_type').size().plot(kind='bar')\n",
    "plt.title('Count of Commute Types')\n",
    "plt.ylabel('Number of Commutes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you say about the data based on these graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current dtype of time_of_day_ts is not great for graphing. It might be a good idea to do something about it. \n",
    "# This function converts this into a decimal between zero and twenty-four.\n",
    "\n",
    "def timestamp_to_decimal(ts):\n",
    "    \"\"\"Convert a timestamp datum into a decimal between zero and twenty-four.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts: pd.Series of datetime.\n",
    "    \"\"\"\n",
    "    return ts.dt.hour + (1/60)*ts.dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our new function on the data and create a new column. Notice that the function is being used to transform \n",
    "# both the training AND testing datasets.\n",
    "train_data_raw['time_of_day'] = timestamp_to_decimal(\n",
    "    train_data_raw['time_of_day_ts'])\n",
    "test_data_raw['time_of_day'] = timestamp_to_decimal(\n",
    "    test_data_raw['time_of_day_ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that time of day is represented by a number from 0 to 24, let's see how many trips are being made throughout \n",
    "# the day.\n",
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['time_of_day'].hist(bins=50)\n",
    "plt.title('Volume of Commutes by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Volume of Commutes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have visualized the variables included in the dataset. Sometimes variables need to be transformed before they can be graphed in a meaningful way. We've looked at locations, commute types, commute times, time of day. The dataset contains information we can use to generate new features. Let's revisit location data because with a source and a destination, we can calculate distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dist_image](https://slideplayer.com/slide/4829376/15/images/8/Some+Euclidean+Distances.jpg \"Euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance is also known as L2.\n",
    "def euclidean_distance(source_x, source_y, target_x, target_y):\n",
    "    return np.sqrt((source_x - target_x)**2 + (source_y - target_y)**2)\n",
    "\n",
    "# Manhattan (or taxicab) distance is also known as L1.\n",
    "def manhattan_distance(source_x, source_y, target_x, target_y):\n",
    "    return np.abs(source_x - target_x) + np.abs(source_y - target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw['euclidean_distance'] = euclidean_distance(\n",
    "    train_data_raw['source_latitude'], train_data_raw['source_longitude'],\n",
    "    train_data_raw['destination_latitude'], train_data_raw['destination_longitude'])\n",
    "test_data_raw['euclidean_distance'] = euclidean_distance(\n",
    "    test_data_raw['source_latitude'], test_data_raw['source_longitude'],\n",
    "    test_data_raw['destination_latitude'], test_data_raw['destination_longitude'])\n",
    "\n",
    "train_data_raw['manhattan_distance'] = manhattan_distance(\n",
    "    train_data_raw['source_latitude'], train_data_raw['source_longitude'],\n",
    "    train_data_raw['destination_latitude'], train_data_raw['destination_longitude'])\n",
    "test_data_raw['manhattan_distance'] = manhattan_distance(\n",
    "    test_data_raw['source_latitude'], test_data_raw['source_longitude'],\n",
    "    test_data_raw['destination_latitude'], test_data_raw['destination_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['euclidean_distance'].hist(bins=50)\n",
    "plt.title('Euclidean Commute Distance')\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Number of Commuters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['manhattan_distance'].hist(bins=50)\n",
    "plt.title('Manhattan Commute Distance')\n",
    "plt.xlabel('Manhattan Distance')\n",
    "plt.ylabel('Number of Commuters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have some information about distance, do you have a guess for which type is related to commute time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOAR features\n",
    "### At this point, we have done some feature engineering with numerical (continuous) data. What do you do with categorical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The computer can't make sense of 'BIKE', 'CAR', etc, but it does understand 0 and 1 and combinations of those digits.\n",
    "# Each of these combinations can be referred to as a level of a particular feature.\n",
    "# Why does it make sense to leave one (level) out? By process of elimination, if a datapoint isn't one of the \n",
    "# n-1 levels, it must be the nth.\n",
    "\n",
    "def create_indicator_features(feature, leave_one_out=True):\n",
    "    # Sort the levels so we always get the same ordering of new features.\n",
    "    levels = list(sorted(np.unique(feature)))\n",
    "    # If we need to leave one out to avoid identifiability issues, we will \n",
    "    # leave out the *last* level, in sorted order.\n",
    "    if leave_one_out:\n",
    "        levels = levels[:-1]\n",
    "    indicator_features = []\n",
    "    for level in levels:\n",
    "        indicator_feature = (feature == level)\n",
    "        indicator_feature_name = \"is_{0}\".format(level)\n",
    "        indicator_features.append(\n",
    "            pd.Series(indicator_feature, \n",
    "                      name=indicator_feature_name, \n",
    "                      index=feature.index,\n",
    "                      dtype=int))\n",
    "    return pd.concat(indicator_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commute_type_features_train = create_indicator_features(train_data_raw['commute_type'])\n",
    "commute_type_features_test = create_indicator_features(test_data_raw['commute_type'])\n",
    "\n",
    "commute_type_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns before moving on.\n",
    "train_data_raw = train_data_raw.drop(['time_of_day_ts', 'commute_type'], axis=1)\n",
    "test_data_raw = test_data_raw.drop(['time_of_day_ts', 'commute_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine our new indicator variables to the original dataframes.\n",
    "train_data_processed = pd.concat([train_data_raw, commute_type_features_train], axis=1)\n",
    "test_data_processed = pd.concat([test_data_raw, commute_type_features_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed.to_csv('../data/train_data_processed.csv')\n",
    "test_data_processed.to_csv('../data/test_data_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit more practice with some python ideas:\n",
    "### Defining functions, using for loops, simplifying your life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
