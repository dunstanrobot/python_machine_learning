{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portions of today's lesson were adapted from materials developed by Susan Fung and Hanna Landrus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fredhutch.io -- Intermediate Python: Machine Learning\n",
    "Fred Hutchinson Cancer Research Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 -- Basic Data Understanding and Preparation: Data Import, Cleaning, Visualization, and Feature Engineering\n",
    "We have access to a set of data describing a variety of features about people's commutes. We're hoping to use these data to predict how long a new person's commute will be given some information about them. Can machine learning help us achieve our goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By the end of today's class, you should be able to:\n",
    "* Import a simple dataset and explore different qualitative and quantitative aspects of the data\n",
    "* Perform simple cleaning and visualization with single variables\n",
    "* Prepare the data for further analysis by engineering new features (columns) from existing numeric and categorical features\n",
    "* Discuss how writing functions and classes can save considerable effort by allowing us to maintain control over inputs and avoid repetitious editing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughout we've scattered pairs of cells like the 2 immediately below. Use them to note your thoughts, answers to questions, and the code you're experimenting with.\n",
    "(remember that you can change the type of a cell by going into command mode (cell highlighted in blue) and pressing `m` for markdown and `y` for code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA, Data Import, Cleaning, Visualization\n",
    "What do we need to do with the data first in order to decide where to go and how to use machine learning thoughtfully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Data exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The % signifies a ipynb \"magic function\". This line allows the figure to be rendered in the notebook next to the\n",
    "# code. \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What versions are you running?\n",
    "print(\"Pandas version:\",pd.__version__)\n",
    "print(\"Numpy version:\",np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data. Remember to pass in the full file path!\n",
    "training_dataset_path = '../data/commute-times-train.csv'\n",
    "testing_dataset_path = '../data/commute-times-test.csv'\n",
    "train_data_raw = pd.read_csv(training_dataset_path, index_col=[0], parse_dates=['time_of_day_ts'])\n",
    "test_data_raw = pd.read_csv(testing_dataset_path, index_col=[0], parse_dates=['time_of_day_ts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick primer to viewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to quickly access docstring on syntax\n",
    "?pd.DataFrame.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the first five rows of data.\n",
    "train_data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other options include using .tail() and .sample(). You can specify how many rows of data to see. The default is 5. \n",
    "train_data_raw.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other methods to use to get some information about your dataset. By default, using .describe() provides some summary\n",
    "# statistics of what kind of data?\n",
    "train_data_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case it is of interest, there are kwargs that will provide additional information.\n",
    "train_data_raw.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a quick way to learn what size dataset you are working with. .shape will return (rows, columns).\n",
    "train_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to know what data types are in your dataset! It's not usually a good idea to make assumptions about \n",
    "# the data you are working with. \n",
    "train_data_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: np.NaN != [None, 0] \n",
    "NaN is very useful because you can leverage vectorized operations in numpy. The data type for each of those values (NaN, None, 0) is different. Think about this when you consider how to impute or otherwise handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missing data in this dataset?\n",
    "train_data_raw.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are NaNs, how many are present in each column?\n",
    "train_data_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Insights Through Quick Vizualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's continue learning about the data with some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we discern any information from lat long?\n",
    "train_data_raw.plot.scatter(x='source_latitude', y='source_longitude', marker='.')\n",
    "plt.title('Source of Commute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw.plot.scatter(x='destination_latitude', y='destination_longitude', marker='.')\n",
    "plt.title('Destination of Commute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show the distribution of commute times.\n",
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['commute_time'].hist(bins=50)\n",
    "plt.title('Histogram of Commute Times')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about a table showing commuters of each type?\n",
    "train_data_raw.groupby('commute_type').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice summary table, but let's graph this out somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw.groupby('commute_type').size().plot(kind='bar')\n",
    "plt.title('Count of Commute Types')\n",
    "plt.ylabel('Number of Commutes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the data based on these graphs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Touch of Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current dtype of `time_of_day_ts` is not great for graphing. It might be a good idea to do something about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts this into a decimal between zero and twenty-four.\n",
    "\n",
    "def timestamp_to_decimal(ts):\n",
    "    \"\"\"Convert a timestamp datum into a decimal between zero and twenty-four.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts: pd.Series of datetime.\n",
    "    \"\"\"\n",
    "    return ts.dt.hour + (1/60)*ts.dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our new function on the data and create a new column. Notice that the function is being used to transform \n",
    "# both the training AND testing datasets.\n",
    "train_data_raw['time_of_day'] = timestamp_to_decimal(\n",
    "    train_data_raw['time_of_day_ts'])\n",
    "test_data_raw['time_of_day'] = timestamp_to_decimal(\n",
    "    test_data_raw['time_of_day_ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that time of day is represented by a number from 0 to 24, let's see how many trips are being made throughout \n",
    "# the day.\n",
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['time_of_day'].hist(bins=50)\n",
    "plt.title('Volume of Commutes by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Volume of Commutes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Touch of Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have visualized certain variables included in the dataset and noted how sometimes variables need to be transformed before they can be graphed in a meaningful way. \n",
    "\n",
    "We've looked at locations, commute types, commute times, time of day. The dataset contains information we can use to generate new features. \n",
    "\n",
    "Let's revisit location data because with a source and a destination, we can calculate distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dist_image](https://slideplayer.com/slide/4829376/15/images/8/Some+Euclidean+Distances.jpg \"Euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance is also known as L2.\n",
    "def euclidean_distance(source_x, source_y, target_x, target_y):\n",
    "    return np.sqrt((source_x - target_x)**2 + (source_y - target_y)**2)\n",
    "\n",
    "# Manhattan (or taxicab) distance is also known as L1.\n",
    "def manhattan_distance(source_x, source_y, target_x, target_y):\n",
    "    return np.abs(source_x - target_x) + np.abs(source_y - target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw['euclidean_distance'] = euclidean_distance(\n",
    "    train_data_raw['source_latitude'], train_data_raw['source_longitude'],\n",
    "    train_data_raw['destination_latitude'], train_data_raw['destination_longitude'])\n",
    "test_data_raw['euclidean_distance'] = euclidean_distance(\n",
    "    test_data_raw['source_latitude'], test_data_raw['source_longitude'],\n",
    "    test_data_raw['destination_latitude'], test_data_raw['destination_longitude'])\n",
    "\n",
    "train_data_raw['manhattan_distance'] = manhattan_distance(\n",
    "    train_data_raw['source_latitude'], train_data_raw['source_longitude'],\n",
    "    train_data_raw['destination_latitude'], train_data_raw['destination_longitude'])\n",
    "test_data_raw['manhattan_distance'] = manhattan_distance(\n",
    "    test_data_raw['source_latitude'], test_data_raw['source_longitude'],\n",
    "    test_data_raw['destination_latitude'], test_data_raw['destination_longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "##### The code above was a lot to read and pretty repetitious.\n",
    "##### We've mentioned the idea of avoiding repeated code where possible, particularly to avoid errors that pop up when making the same edit in multiple places.\n",
    "##### What could we do to avoid having to type the same code multiple times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this template to start if you want. \n",
    "# The docstring format below is one common convention.\n",
    "# There are other good conventions represented elsewhere in FH materials.\n",
    "# None of these is exactly right or wrong, but the important things are consistency and clarity.\n",
    "def create_distance_features(df):\n",
    "    \"\"\"Describe what the function does here.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Important notes about our argument df here.\n",
    "    \"\"\"\n",
    "    pass\n",
    "# I've included a possible solution near the bottom of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the counts for different distances using our new distance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['euclidean_distance'].hist(bins=50)\n",
    "plt.title('Euclidean Commute Distance')\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Number of Commuters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "train_data_raw['manhattan_distance'].hist(bins=50)\n",
    "plt.title('Manhattan Commute Distance')\n",
    "plt.xlabel('Manhattan Distance')\n",
    "plt.ylabel('Number of Commuters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some information about distance, do you have a guess for which type is related to commute time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOAR features\n",
    "### At this point, we have done some feature engineering with numerical (continuous) data. What do you do with categorical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The computer can't typically make sense of 'BIKE', 'CAR', etc, but it does understand \n",
    "# 0 and 1 and combinations of those digits.\n",
    "# Each of these combinations can be referred to as a level or category of a particular feature.\n",
    "# Why does it make sense to leave one (level) out? By process of elimination, if a datapoint isn't \n",
    "# one of the n-1 levels, it must be the nth.\n",
    "\n",
    "def create_indicator_features(feature, leave_one_out=True):\n",
    "    # Sort the levels so we always get the same ordering of new features.\n",
    "    levels = list(sorted(np.unique(feature)))\n",
    "    # If we need to leave one out to avoid identifiability issues, we will \n",
    "    # leave out the *last* level, in sorted order.\n",
    "    if leave_one_out:\n",
    "        levels = levels[:-1]\n",
    "    indicator_features = []\n",
    "    for level in levels:\n",
    "        indicator_feature = (feature == level)\n",
    "        indicator_feature_name = \"is_{0}\".format(level)\n",
    "        indicator_features.append(\n",
    "            pd.Series(indicator_feature, \n",
    "                      name=indicator_feature_name, \n",
    "                      index=feature.index,\n",
    "                      dtype=int))\n",
    "    return pd.concat(indicator_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commute_type_features_train = create_indicator_features(train_data_raw['commute_type'])\n",
    "commute_type_features_test = create_indicator_features(test_data_raw['commute_type'])\n",
    "\n",
    "commute_type_features_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns before moving on.\n",
    "train_data_raw = train_data_raw.drop(['time_of_day_ts', 'commute_type'], axis=1)\n",
    "test_data_raw = test_data_raw.drop(['time_of_day_ts', 'commute_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine our new indicator variables to the original dataframes.\n",
    "train_data_processed = pd.concat([train_data_raw, commute_type_features_train], axis=1)\n",
    "test_data_processed = pd.concat([test_data_raw, commute_type_features_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our processed data to a directory\n",
    "train_data_processed.to_csv('../data/train_data_processed.csv')\n",
    "test_data_processed.to_csv('../data/test_data_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit more practice with some python ideas: \n",
    "* Defining functions, using for loops, simplifying your life..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible solution to one of the questions above. Did yours look something like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distance_features(df):\n",
    "    \"\"\"Add 'euclidean_distance' and 'manhattan_distance' feaures to a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.Dataframe with features 'source_latitude', 'source_longitude', 'destination_latitude', \n",
    "    and 'destination_longitude'.\n",
    "    \"\"\"\n",
    "    coordinates = [df['source_latitude'], df['source_longitude'], \n",
    "                   df['destination_latitude'], df['destination_longitude']]\n",
    "    df['euclidean_distance'] = euclidean_distance(coordinates)\n",
    "    df['manhattan_distance'] = manhattan_distance(coordinates)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even more practice with python ideas:\n",
    "* Using classes to maintain desired behavior within pipelines..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, we'll want the ability to include our functions in a workflow to transform data in a consistent way, particularly when performing transformations separately on the **train** and **test** partitions of a dataset. It's common maintain control of inputs and outputs by writing a wrapper function to call functions in a specific order. \n",
    "\n",
    "Another good way to maintain consistent control is to write classes that perform the necessary work while integrating nicely with existing pipeline elements, such as those from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we defined the *function* `create_indicator_features` to allow us to convert a single categorical variable into a number of binary indicator features (sometimes called dummy variables) for the different levels/categories in the original column.\n",
    "\n",
    "The code below is from an early attempt to create a version of `pandas.get_dummies` that maintains the **ordering** of our new features regardless of differences in levels for **train** and **test** sets. I chose to write it as a *class* in order to integrate more easily with pipelines calling the `fit` and `transform` methods found throughout `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "class DummyMaker:\n",
    "    \"\"\"Class takes a categorical variable and returns a DataFrame with a column\n",
    "    for each category having values of 0 or 1 for each row.\n",
    "    A string passed to the constructor will become a prefix for dummy\n",
    "    column names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prefix=None):\n",
    "        self.levels = None\n",
    "        if prefix is None:\n",
    "            self.prefix = \"\"\n",
    "        else:\n",
    "            self.prefix = prefix + \"__\"\n",
    "        self.colnames = None\n",
    "\n",
    "    def fit(self, categorical_column):\n",
    "        \"\"\"Store the levels from categorical_column, a pd.Series (df[colname]).\n",
    "        unique_cats is a list of unique categories in that column.\n",
    "        self.colnames creates dummy column names with optional prefix.\n",
    "        \"\"\"\n",
    "        unique_cats = np.unique(categorical_column)\n",
    "        self.levels = unique_cats\n",
    "        self.colnames = [self.prefix + level.replace(\" \", \"_\")\n",
    "                         for level in self.levels]\n",
    "\n",
    "    def transform(self, categorical_column, k_minus_one=False):\n",
    "        \"\"\"If k_minus_one=True, the column representing the first unique category\n",
    "        is dropped.\n",
    "        The indexing of categorical_column is preserved in the new DataFrame.\n",
    "        \"\"\"\n",
    "        num_rows = len(categorical_column)\n",
    "        num_features = len(self.levels)\n",
    "        dummies = np.zeros(shape=(num_rows, num_features))\n",
    "        for i, value in enumerate(self.levels):\n",
    "            dummies[:, i] = (categorical_column == value).astype(int)\n",
    "        if k_minus_one == True:\n",
    "            return pd.DataFrame(dummies[:, 1:], columns=self.colnames[1:],\n",
    "                                index=categorical_column.index)\n",
    "        else:\n",
    "            return pd.DataFrame(dummies, columns=self.colnames,\n",
    "                                index=categorical_column.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "##### Do any issues pop out that might arise from the use of the above code in a production environment?\n",
    "##### How could you improve the above code to avoid those issues?\n",
    "##### Could you write a series of tests that would allow you to assess whether the above code functions properly in all the important cases?\n",
    "##### How would you go about writing your own version of a dummy/indicator variable maker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
